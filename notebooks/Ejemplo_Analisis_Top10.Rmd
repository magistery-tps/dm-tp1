---
title: "TP1 - Spotify"
author: " Flavia Felicioni"
date: "9 de mayo de 2021"
output: html_document 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

<br>
<br>

<center> <h1>Carga de datos</h1> </center> 
<br>
Se cargan los datos de la colección track_features_top_10.
db.getCollection('track_weekly_top_10').count()=1570
y track_features_top_10 tiene 1148 filas. 

En la query db.getCollection('track_weekly_top_10').distinct('url') nos dan 294 urls distintas.
Pero cuando se hizo lookup por url (asumiendo que cada url identifica la canción escuchada) resultaron 197, es decir se perdieron casi 100 urls (equivalentemente canciones) por haber hecho la búsqueda por url (información anecdótica).

<br>

### <span style="color:darkred">Datasets</span>
<br>


```{r echo=TRUE,eval=TRUE}
library(pacman)
library(mongolite)
library(jsonlite)
library(tidyverse)
library(dplyr)#Paquete para manipular datos

```

```{r echo=TRUE,eval=TRUE}

#cargo coleccion, la paso a dataframe y elimino duplicados
track_features_top_10 <- mongo(db="spotify",collection='track_features_top_10')
result <- track_features_top_10$find('{}')
ds_tf10<-result[!duplicated(result),]
rm(result) #borro result
rm(track_features_top_10)

```

A partir de aquí se pone en formato todas las fechas del dataframe ds_tf10 para poder operar después:

```{r echo=TRUE,eval=TRUE}

#detecte que hay fechas de album_release_date  que estan incompletas, solo con el año
#cuando pasa esto completo las fechas poniendo 1 de enero de cada año

ds_tf10$album_release_date[str_length(ds_tf10$album_release_date)==4]<-paste0(ds_tf10$album_release_date[str_length(ds_tf10$album_release_date)==4],"-01-01")
#ahora si puedo convertir a formato date
ds_tf10$album_release_date<-as.Date(ds_tf10$album_release_date) 

#convierto las columnas a tipo date
ds_tf10$week_start<-as.Date(ds_tf10$week_start)#,format="%m/%d/%Y") esto era para el csv
ds_tf10$week_end<-as.Date(ds_tf10$week_end)

```
Ejemplo de agrupamiento
```{r echo=TRUE,eval=TRUE}
ds<-ds_tf10[,c(1,2,3,4,6,13,15,16,17,18,19,20,21,22)]

#agrupo por url, saco fechas de ingreso al chart y de egreso sumo las reproducciones y el resto de parametros hago promedio
grupos<-ds %>%  group_by(url) %>% 
summarise(minimo=min(position),maximo=max(position),fecha_album=min(album_release_date),week_in=min(week_start)
          ,week_out=max(week_end),cant_rep=sum(reproductions),danceability=mean(danceability),energy=mean(energy)
          ,loudness=mean(loudness),speechiness=mean(speechiness),acousticness=mean(acousticness),instrumentalness=mean(instrumentalness)
          ,liveness=mean(liveness),valence=mean(valence),n=n())

```

Algunas operaciones que se pueden hacer y graficas (de ejemplo)
```{r echo=TRUE,eval=TRUE}

valor=as.Date("2018-01-01")

# la cantidad de albumnes cuyas canciones llegaron al top 10 que fueron lanzados despues de 2018 (1101)
length(ds_tf10$album_release_date[ds_tf10$album_release_date>valor])


plot(grupos$week_in-grupos$fecha_album)
sum(grupos$n)

plot(grupos$week_in-grupos$fecha_album,grupos$n)
hist(grupos$n)
boxplot(grupos$n)


deltat=ds_tf10$week_start-ds_tf10$album_release_date
#deltat[deltat>0]
#hay cuatro fechas que dan negativas
length(deltat[deltat>0])

plot(deltat,ds_tf10$position)

gp1<-grupos[grupos$week_in-grupos$fecha_album<500,]
plot(gp1$week_in-gp1$fecha_album,gp1$n)

plot(grupos$week_in,grupos$danceability)


novedad<-grupos$fecha_album<valor
plot.new()
datos=data.frame(table(novedad,grupos$fecha_album))

barplot(table(novedad,grupos$minimo),xlab="position",ylab="cantidad canciones",col=rainbow(2))
legend("topright",cex=1,title="leyenda",c("new","old"),fill=rainbow(2),horiz=F) # asigna leyendas en posición horizontal

deltat=grupos$week_in-grupos$fecha_album
deltat[deltat>=0]

```
<br>

<br>
....

<br>
